---
title: "pr10-web_scrapping"
author: "강현승"
date: "2022-11-10"
output: html_document
---

```{r setup, include=FALSE}
q2_answer = "
제일 겉부분 for 문은 `article.offset`을 조작해 공지사항 자체 페이지를 넘어가게 하기 위해서 쓰입니다.
예) 0-10 번째 행을 담고 있는 페이지를 가져오도록 하기 위해
for 문 내부에 있는 for 문은 응답받은 페이지 속 행 하나하나를 가져오기 위해 쓰입니다.
예) 첫 번째 URL 응답 본문 속 0-10 번째 행 속 0 번째 행을 가져오기 위해
"
```

# rvest를 이용하여 아주대 공지사항 크롤링

```{r}
library(rvest) #아래 코드에서 rvest 패키지를 사용하도록 하겠습니다.

title <- NULL
dept <- NULL
date <- NULL # 미리 변수를 설정합니다. 그래야 아래 c() 함수로 합친 걸 이 변수로 넣을 수 있습니다.
for (i in 1:2) { # 두 번째 페이지까지 봅니다.
  url <-
    "https://www.ajou.ac.kr/kr/ajou/notice.do?mode=list&&articleLimit=10&article.offset="
  urls <- paste0(url, (i - 1) * 10) # article.offset는 0부터 시작하는 쿼리로 화상에 표시되는 첫 행의 순번을 지정합니다.
  html_source <- read_html(urls) # 위 코드에서 두 개의 URL, 0-10, 10-20 번째 행을 나타내는 페이지를 가리키는 URL로 HTTP요청, http response body를 해석하여 저장합니다.
  for (i in 1:10) { # 위에서 해석한 공지사항 페이지 두 장의 글 목록을 불러옵니다. articleLimit 쿼리가 10이었습니다. 따라서 공지사항 목록 페이지 한 장 당 10 개행이 표시됩니다.
    T.selector <-
      paste0(
        "#cms-content > div > div > div.bn-list-common02.type01.bn-common-cate > table > tbody > tr:nth-child(",
        i,
        ") > td.b-td-left > div > a"
      ) # 각 글의 제목 부분 태그를 지칭하는 css selector를 변수에 넣습니다.
    title.nodes <- html_nodes(html_source, T.selector) # 상기 css selector로 해석한 문서 속 element를 찾습니다.
    T.title <- html_text(title.nodes) # 상기 찾은 element의 글자를 추출합니다.
    title <- c(title, T.title) # 상기 추출한 글자를 아주 위에서 할당한 변수 title과 합쳐 다시 title에 넣습니다.
    D.selector <-
      paste0(
        "#cms-content > div > div > div.bn-list-common02.type01.bn-common-cate > table > tbody > tr:nth-child(", i, ") > td:nth-child(5)"
      ) # 각 글의 부서 부분 태그를 지칭하는 css selector를 변수에 넣습니다.
    dept.nodes <-
      html_nodes(html_source, D.selector) # 상기 css selector로 해석한 문서 속 element를 찾습니다.
    T.dept <- html_text(dept.nodes) # 상기 찾은 element의 글자를 추출합니다.
    dept <- c(dept, T.dept) # 상기 추출한 글자를 아주 위에서 할당한 변수 dept과 합쳐 다시 dept에 넣습니다.
    date.selector <-
      paste0(
        "#cms-content > div > div > div.bn-list-common02.type01.bn-common-cate > table > tbody > tr:nth-child(", i, ") > td:nth-child(6)"
      ) # 각 글의 일자 부분 태그를 지칭하는 css selector를 변수에 넣습니다.
    date.nodes <-
      html_nodes(html_source, date.selector) # 상기 css selector로 해석한 문서 속 element를 찾습니다.
    T.date <-
      html_text(date.nodes) # 상기 찾은 element의 글자를 추출합니다.
    date <- c(date, T.date) # 상기 추출한 글자를 아주 위에서 할당한 변수 date과 합쳐 다시 date에 넣습니다.
  }
}
```

```{r}
ajou.notice = data.frame(title, date, dept) # 위 청크에서 만든 벡터들을 묶어 하나의 데이타프레임으로 만듭니다.

ajou.notice[, 1] = gsub("\n", "", ajou.notice[, 1]) # 위 데이터프레임의 title 부분의 탭, 개행문자 제거합니다.
ajou.notice[, 1] = gsub("\t", "", ajou.notice[, 1])
ajou.notice[, 1] = gsub("\r", "", ajou.notice[, 1])
ajou.notice$title = trimws(ajou.notice$title) # 양 끝단 공백을 제거합니다. - trim

head(ajou.notice) # 상위 6개 행만 표시합니다.
```

# Rselenium을 이용한 유튜브 제목 크롤링

```{r}
# library(devtools)
# install_github('khskeb0513/RSelenium')
library(RSelenium)
library(httr)
library(dplyr)

youtube_remDR = remoteDriver(browserName = "chrome") # 사용할 브라우저
youtube_remDR$open(silent = T) # 서버에 연결
title_youtube = "이게뭐람"

# 해당 홈페이지로 이동
youtube_remDR$navigate(paste0(
  "https://www.youtube.com/results?search_query=",
  title_youtube
))

# "이게뭐람" 페이지 클릭
more = youtube_remDR$findElement(using = "css", value = "#text-container")
more$clickElement()
Sys.sleep(2)

# 동영상 탭 클릭
video = youtube_remDR$findElement(using = "css", value = "#tabsContent > tp-yt-paper-tab:nth-child(4) > div")
video$clickElement()
Sys.sleep(2)

# 페이지 소스 가져오기
html = youtube_remDR$getPageSource()[[1]]
html = read_html(html)

# 선택된 노드를 텍스트화
youtube_title = html %>% html_nodes("#video-title") %>% html_text()

# 데이터 정제
youtube_title = gsub("\n", "", youtube_title) # 데이터 정제 1
youtube_title = trimws(youtube_title) # 데이터 정제 2

# 필요 있는 부분만 가져오기
youtube_title = youtube_title[26:length(youtube_title)]

head(youtube_title)
```

### 문제 1

위의 코드는 아주대 공지사항을 크롤링한 코드입니다. 모든 코드의 주석을 상세하게 달아주시기 바랍니다.

```{r}
"네"
```

### 문제 2

위의 문제에서 이중반복문을 돌리는 이유가 무엇일까요? 이유를 추론하여 적어주세요.

```{r}
strsplit(q2_answer, '\n')
```

### 문제 3

> Selenium을 이용해야만 크롤링할 수 있는 사이트와 크롤링하려는 이유를 적어주세요.

아주Bb를 크롤링하려 합니다.<br>
Blackboard 앱의 push notification 이 정상적으로, 혹은 적시에 오지 않는 경우가 많습니다.<br>
이로 인해 학생은 과제를 인지하지 못하는 경우가 있습니다.<br>
정기적으로 pulling 하여 적기에 사용자가 과제를 시작, 제출, 놓치지 않을 수 있도록 확인할 수 있는 서비스를 고안해보았습니다.

아래 같이 과제를 불러왔지만, 페이지에 접근하기 위해 HTTP 요청 시 header에 cookie 값을 가져오기 위해선 `sso.ajou.ac.kr`에서 SAML 인증 후 인증을 요청한 사용자 명의로 인증된 URL을 가져와야 함을 알았습니다.

이 과정을 POST 요청으로 모두 모사하기에는 복잡하므로 개인적인 목적으로, 혹은 완성도 낮은 mvp를 출시 할 때는 selenium을 이용하여 실제 사용자가 로그인하는 과정을 재현하는 것이 적합하다고 판단합니다.


```{r}
library(rvest)
library(httr)
library(readr)
library(stringr)

# 부득이하게 쿠키값은 보이지 않게 처리했습니다.
bb_cookie = read_file('bb_cookie.txt')

handle_reset("https://eclass2.ajou.ac.kr")
bb_assignmets_html = read_html(content(
  GET(
    'https://eclass2.ajou.ac.kr/webapps/bb-mygrades-BB5ff5398b9f3ea/myGrades?course_id=_79101_1&stream_name=mygrades',
    add_headers(cookie = bb_cookie)
  ),
  as = 'raw'
)) # R프 성적확인 페이지입니다. 과제 공시 시 이 페이지에서 확인 가능함을 확인했습니다.

assignmet_titles = str_replace_all(trimws(html_text(
  html_nodes(bb_assignmets_html, 'div.cell.gradable > *:nth-child(1)')
)), ' ', '')
assignmet_dates = str_replace_all(trimws(html_text(
  html_nodes(bb_assignmets_html, 'div.cell.gradable > *:nth-child(2)')
)), ' ', '')

tibble(assignmet_titles, assignmet_dates) # 이와 같이 과제를 불러올 수 있음을 확인했습니다.
```

### 문제 4

아래의 코드는 판례데이터를 다운받기 위해 자동화한 코드입니다. 아래의 코드를 실행하고 R서버에서 워드클라우드를 그려주시기 바랍니다.

혹시 크롤링이 되지 않는 경우 엑셀파일을 올려두니 이를 활용해서 워드클라우드 실습을 진행해주시기 바랍니다.

```{r}
# install.packages('RSelenium')
library(RSelenium)
library(rvest)

search_keyword = '산업재해'

remDR = remoteDriver(browserName = "chrome")
remDR$open(silent = T)

# 팝업창 해소를 위한 쿠키 설정
url = "https://glaw.scourt.go.kr/"
remDR$navigate(url)
remDR$addCookie('popupGongji731', 'true', domain = 'scourt.go.kr')
Sys.sleep(1)

# 실지 주소로 이동
url = "https://glaw.scourt.go.kr/wsjo/panre/sjo060.do"
remDR$navigate(url)
Sys.sleep(1)

# 검색 입력

# 1. 검색 입력창 찾기
search_txt = remDR$findElement(using = "css selector", "#search > div.input_area_wrap > fieldset > input")

# 2. 검색 입력창 클릭
search_txt$clickElement()
Sys.sleep(1)

# 3. 검색어 입력
search_txt$sendKeysToElement(list(paste0('*', search_keyword, '*')))

# 4.검색 클릭창 찾기
search_button = remDR$findElement(using = "css selector", "#srch_img")

# 5. 검색 클릭창 클릭
search_button$clickElement()
Sys.sleep(1)

# 6. 최신 순 정렬
remDR$findElement(using = 'css selector', 'a[name=sort_sngo_day]')$clickElement()
Sys.sleep(5)

# 6. 다음 페이지 클릭
# next_page = remDR$findElement(
#   using = "css selector",
#   "#tabwrap > div > div > div.tab_contents > div.tab_util.border_bottm > div.list_option > div > fieldset > p > a:nth-child(3) > img"
# )
# next_page$clickElement()

# 클릭을 하고 탭을 바꾸기 위함.
myswitch = function(remDr, windowId)
{
  qpath = sprintf("%s/session/%s/window",
                  remDr$serverURL,
                  remDr$sessionInfo[["id"]])
  remDr$queryRD(qpath, "POST", qdata = list(handle = windowId))
}

panre_df = data.frame()

for (result_num in 1:20) {
  panre_xpath = paste0(
    '/html/body/div[1]/div[4]/div/div/div[2]/div[2]/table/tbody/tr[',
    result_num,
    ']/td[2]/dl/dt/a[1]'
  )
  panre_ = remDR$findElement(using = 'xpath', panre_xpath)
  panre_$clickElement()
  Sys.sleep(1)
  
  remDR$getCurrentWindowHandle()
  remDR$getWindowHandles()
  
  check_handle = F
  count = 0
  while (!check_handle | count > 20) {
    count = count + 1
    windows_handles = remDR$getWindowHandles()
    if (length(windows_handles) < 2) {
      Sys.sleep(1)
    } else{
      check_handle = T
    }
  }
  
  myswitch(remDR, windows_handles[[2]])
  remDR$getCurrentWindowHandle()
  panre_docs = remDR$getPageSource()[[1]]
  panre_docs_html = read_html(panre_docs)
  
  panre_id = html_text(html_nodes(panre_docs_html, "p.stt"))[[1]]
  panre_id
  
  panre_title = html_text(html_nodes(panre_docs_html, "div.tit_area.common"))
  panre_title
  
  panre_text = html_text(html_nodes(panre_docs_html, 'div.con_scroll_area.panre'))
  panre_text
  
  panre_chj = html_text(html_nodes(panre_docs_html, 'p.areaBmunChjJomun>a'))
  
  if (length(panre_chj) > 0) {
    result_df = data.frame(panre_id, panre_chj, panre_title, panre_text)
    panre_df = rbind(panre_df, result_df)
  }
  
  remDR$closeWindow()
  myswitch(remDR, windows_handles[[1]])
}

remDR$close()

# 텍스트 크롤링 결과는 엑셀로 저장
# library(openxlsx)
# openxlsx::write.xlsx(panre_df, "ajou_panre_df.xlsx")
```

## 워드클라우드 생성

```{r}
# install.packages('wordcloud2')
# install.packages('RcppMeCab')
# install.packages('tidytext')
# install.packages('dplyr')
# install.packages('stopwords')
# install.packages(stringr)
library(wordcloud2)
library(RcppMeCab)
library(tidytext)
library(dplyr)
library(stopwords)
library(stringr)

# panre_df = read.xlsx("panre_df.xlsx")

set.seed(1234)
panre_pos = panre_df %>%
  unnest_tokens(word, panre_text, token = posParallel) %>%
  filter(str_detect(word, '/nnp')) # 고유명사
  # filter(str_detect(word, '/nng')) # 일반명사
  # filter(str_detect(word, '/nng|/nnp')) # 일반명사와 일반명사

panre_pos$word = sapply(str_split(panre_pos$word, '/'), "[[", 1)

panre_pos = panre_pos %>%
  anti_join(tibble(word = stopwords('ko', 'marimo'))) %>% # 패키지 활용 불용어 삭제
  anti_join(tibble(word = c(
    '경우', '원고', '피고', '사건', '판결', '항', '법', '위', '기준', '법원'
  ))) %>% # 직접 불용어 삭제
  count(word, sort = T)

head(panre_pos)

panre_pos %>% wordcloud2(shape = 'cardioid')
```

## 도전문제

유튜브에서 셀레니움을 활용하여 제목 이외에 다른 정보들을 크롤링해주시기 바랍니다.

> 조회수를 크롤링 하였습니다.

```{r}
youtube_remDR$getCurrentUrl() # 위에서 생성한 chrome process 재사용
youtube_html = read_html(youtube_remDR$getPageSource()[[1]])
youtube_remDR$closeWindow()

acc_counts = tibble(text = html_text(
  html_nodes(
    youtube_html,
    'div.ytd-video-meta-block > span.ytd-video-meta-block'
  )
)) %>% filter(str_detect(text, '조회수'))

acc_counts$text = sapply(str_split(acc_counts$text, ' '), "[[", 2)
acc_counts$num_unit = 1
acc_counts$num_unit[str_detect(acc_counts$text, '천')] = 1000
acc_counts$num_unit[str_detect(acc_counts$text, '만')] = 10000
acc_counts$text_numerized = as.numeric(gsub('천|만|회', '', acc_counts$text))
acc_counts$acc_count = acc_counts$text_numerized * acc_counts$num_unit

head(acc_counts)
```